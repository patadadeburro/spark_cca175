{"cells":[{"cell_type":"markdown","source":["# CCA175 Practice Exam\n* Exam    : CCA175\n* Title   : CCA Spark and Hadoop Developer Exam\n* Vendor  : Cloudera\n* Version : V12.35\n\nIT Certification Guaranteed, The Easy Way!"],"metadata":{}},{"cell_type":"markdown","source":["## NO.30 CORRECT TEXT\n### Problem Scenario 28 : \n\nYou need to implement near real time solutions for collecting information\nwhen submitted in file with below\nData\necho \"IBM,100,20160104\" >> /tmp/spooldir2/.bb.txt\necho \"IBM,103,20160105\" >> /tmp/spooldir2/.bb.txt\nmv /tmp/spooldir2/.bb.txt /tmp/spooldir2/bb.txt\nAfter few mins\necho \"IBM,100.2,20160104\" >> /tmp/spooldir2/.dr.txt\necho \"IBM,103.1,20160105\" >> /tmp/spooldir2/.dr.txt\nmv /tmp/spooldir2/.dr.txt /tmp/spooldir2/dr.txt\nYou have been given below directory location (if not available than create it) /tmp/spooldir2\n.\nAs soon as file committed in this directory that needs to be available in hdfs in\n/tmp/flume/primary as well as /tmp/flume/secondary location.\nHowever, note that/tmp/flume/secondary is optional, if transaction failed which writes in this\ndirectory need not to be rollback.\nWrite a flume configuration file named flumeS.conf and use it to load data in hdfs with following\nadditional properties .\n\n1 . Spool /tmp/spooldir2 directory\n2 . File prefix in hdfs sholuld be events\n3 . File suffix should be .log\n4 . If file is not committed and in use than it should have _ as prefix.\n5 . Data should be written as text to hdfs"],"metadata":{}},{"cell_type":"markdown","source":["Answer:\nSee the explanation for Step by Step Solution and configuration.\nExplanation:\nSolution :\nStep 1 : Create directory mkdir /tmp/spooldir2\nStep 2 : Create flume configuration file, with below configuration for source, sink and channel and\nsave it in flume8.conf.\nagent1 .sources = source1\nagent1.sinks = sink1a sink1bagent1.channels = channel1a channel1b\nagent1.sources.source1.channels = channel1a channel1b\nagent1.sources.source1.selector.type = replicating\nagent1.sources.source1.selector.optional = channel1b\nagent1.sinks.sink1a.channel = channel1a\nagent1 .sinks.sink1b.channel = channel1b\nagent1.sources.source1.type = spooldir\nagent1 .sources.sourcel.spoolDir = /tmp/spooldir2\nagent1.sinks.sink1a.type = hdfs\nagent1 .sinks, sink1a.hdfs. path = /tmp/flume/primary\nagent1 .sinks.sink1a.hdfs.tilePrefix = events\nagent1 .sinks.sink1a.hdfs.fileSuffix = .log\nagent1 .sinks.sink1a.hdfs.fileType = Data Stream\nagent1 .sinks.sink1b.type = hdfs\nagent1 .sinks.sink1b.hdfs.path = /tmp/flume/secondary\nagent1 .sinks.sink1b.hdfs.filePrefix = events\nagent1.sinks.sink1b.hdfs.fileSuffix = .log\nagent1 .sinks.sink1b.hdfs.fileType = Data Stream\nagent1.channels.channel1a.type = file\nagent1.channels.channel1b.type = memory\nstep 4 : Run below command which will use this configuration file and append data in hdfs.\nStart flume service:\nflume-ng agent -conf /home/cloudera/flumeconf -conf-file\n/home/cloudera/flumeconf/flume8.conf --name age\nStep 5 : Open another terminal and create a file in /tmp/spooldir2/\necho \"IBM,100,20160104\" > /tmp/spooldir2/.bb.txt\necho \"IBM,103,20160105\" > /tmp/spooldir2/.bb.txt mv /tmp/spooldir2/.bb.txt\n/tmp/spooldir2/bb.txt\nAfter few mins\necho \"IBM.100.2,20160104\" >/tmp/spooldir2/.dr.txt\necho \"IBM,103.1,20160105\" > /tmp/spooldir2/.dr.txt mv /tmp/spooldir2/.dr.txt\n/tmp/spooldir2/dr.txt"],"metadata":{}},{"cell_type":"markdown","source":["#### Answer"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"p30","notebookId":3939867850322609},"nbformat":4,"nbformat_minor":0}
