{"cells":[{"cell_type":"markdown","source":["# CCA175 Practice Exam\n* Exam    : CCA175\n* Title   : CCA Spark and Hadoop Developer Exam\n* Vendor  : Cloudera\n* Version : V12.35\n\nIT Certification Guaranteed, The Easy Way!"],"metadata":{}},{"cell_type":"markdown","source":["## NO.36 CORRECT TEXT\n### Problem Scenario 24 : \nYou have been given below comma separated employee information.\nData Set:"],"metadata":{}},{"cell_type":"code","source":["name,salary,sex,age\nalok,100000,male,29\njatin,105000,male,32\nyogesh,134000,male,39\nragini,112000,female,35\njyotsana,129000,female,39\nvalmiki,123000,male,29"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Requirements:\n\nUse the `netcat` service on port 44444, and nc above data line by line. \n\nPlease do the following activities.\n\n1. Create a flume conf file using fastest channel, which write data in hive warehouse directory, in a\ntable called `flumemaleemployee` (Create hive table as well tor given data).\n2. While importing, make sure only male employee data is stored."],"metadata":{}},{"cell_type":"markdown","source":["#### Answer"],"metadata":{}},{"cell_type":"code","source":["\nSee the explanation for Step by Step Solution and configuration.\nExplanation:\nStep 1 : Create hive table for flumeemployee.'\nCREATE TABLE flumemaleemployee\n(\nname string,\nsalary int,\nsex string,\nage int\n)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n\nstep 2 : Create flume configuration file, with below configuration for source, sink and channel and\nsave it in flume4.conf.\n\n#Define source , sink, channel and agent.\nagent1 .sources = source1\nagent1 .sinks = sink1\nagent1 .channels = channel1\n# Describe/configure source1\nagent1 .sources.source1.type = netcat\nagent1 .sources.source1.bind = 127.0.0.1\nagent1.sources.sourcel.port = 44444\n#Define interceptors\nagent1.sources.source1.interceptors=il\nagent1 .sources.source1.interceptors.i1.type=regex_filter\nagent1 .sources.source1.interceptors.i1.regex=female\nagent1 .sources.source1.interceptors.i1.excludeEvents=true\n## Describe sink1\nagent1 .sinks, sinkl.channel = memory-channel\nagent1.sinks.sink1.type = hdfs\nagent1 .sinks, sinkl. hdfs. path = /user/hive/warehouse/flumemaleemployee hdfs-agent.sinks.hdfswrite.\nhdfs.writeFormat=Text agentl .sinks.sink1.hdfs.fileType = Data Stream\n# Now we need to define channel1 property.\nagent1.channels.channel1.type = memory\nagent1.channels.channell.capacity = 1000\nagent1.channels.channel1.transactionCapacity = 100\n# Bind the source and sink to the channel\n\nagent1 .sources.source1.channels = channel1\nagent1 .sinks.sink1.channel = channel1\n\nstep 3 : Run below command which will use this configuration file and append data in hdfs.\nStart flume service:\nflume-ng agent -conf /home/cloudera/flumeconf -conf-file\n/home/cloudera/flumeconf/flume4.conf --name agentl\n\nStep 4 : Open another terminal and use the netcat service, nc localhost 44444\n\nStep 5 : Enter data line by line.\nalok,100000,male,29\njatin,105000,male,32\nyogesh,134000,male,39\nragini,112000,female,35\njyotsana,129000,female,39\nvalmiki.123000.male.29\n\nStep 6 : Open hue and check the data is available in hive table or not.\n\nStep 7 : Stop flume service by pressing ctrl+c\n\nStep 8 : Calculate average salary on hive table using below query. You can use either hive command\nline tool or hue. select avg(salary) from flumeemployee;"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["create hive table"],"metadata":{}},{"cell_type":"code","source":["CREATE TABLE flumemaleemployee(\n  name   STRING,\n  salary INT,\n  sex    STRING,\n  age    INT\n)\nROW FORMAT DELIMITED \nFIELDS TERMINATED BY ',';"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["flume config file p36.conf"],"metadata":{}},{"cell_type":"code","source":["a1.sources  = r1\na1.sinks    = k1\na1.channels = c1\n\n#sources\na1.sources.r1.type = netcat\na1.sources.r1.bind = 127.0.0.1\na1.sources.r1.port = 44444\n\n#Define interceptors\na1.sources.r1.interceptors                 = il\na1.sources.r1.interceptors.i1.type         = regex_filter\na1.sources.r1.interceptors.i1.regex        = female\na1.sources.r1.interceptors.i1.excludeEvents= true\n\n## Describe sink1\na1.sinks.k1.type                   = hdfs\na1.sinks.k1.hdfs.path              = /user/hive/warehouse/flumemaleemployee \na1.sinks.k1.hdfs.writeFormat       = Text \na1.sinks.k1.hdfs.fileType          = DataStream\n\n# Now we need to define channel1 property.\na1.channels.c1.type                = memory\na1.channels.1.capacity             = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels             = c1\na1.sinks.k1.channel                = c1"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["start flume agent"],"metadata":{}},{"cell_type":"code","source":["flume-ng agent\n--conf /home/cloudera/flumeconf \n--conf-file /home/cloudera/flumeconf/flume4.conf \n--name agentl"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":14}],"metadata":{"name":"p36","notebookId":445118608749331},"nbformat":4,"nbformat_minor":0}
