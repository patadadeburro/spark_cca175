{"cells":[{"cell_type":"markdown","source":["# CCA175 Practice Exam\n* Exam    : CCA175\n* Title   : CCA Spark and Hadoop Developer Exam\n* Vendor  : Cloudera\n* Version : V12.35\n\nIT Certification Guaranteed, The Easy Way!"],"metadata":{}},{"cell_type":"markdown","source":["## NO.40 CORRECT TEXT\nProblem Scenario 37 : ABCTECH.com has done survey on their Exam Products feedback using a web\nbased form. With the following free text field as input in web ui.\n\n| Field | Data Type |\n|-|-|\n| Name              |  String |\n| Subscription Date |  String |\n| Rating            |  String |"],"metadata":{}},{"cell_type":"markdown","source":["And servey data has been saved in a file called `p40/feedback.txt`"],"metadata":{}},{"cell_type":"code","source":["Christopher|Jan 11, 2015|5\nKapil|11 Jan, 2015|5\nThomas|6/17/2014|5\nJohn|22-08-2013|5\nMithun|2013|5\nJitendra||5"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Write a spark program using regular expression which will filter all the valid dates and save in two\nseparate file (good record and bad record)"],"metadata":{}},{"cell_type":"markdown","source":["#### Answer"],"metadata":{}},{"cell_type":"markdown","source":["See the explanation for Step by Step Solution and configuration.\nExplanation:\nSolution :\n\nStep 1 : Create a file first using Hue in hdfs.\n\nStep 2 : Write all valid regular expressions sysntex for checking whether records are having valid dates or not.\nval regl =......(\\d+)\\s(\\w{3})(,)\\s(\\d{4}).......r//11 Jan, 2015\nval reg2 =......(\\d+)(U)(\\d+)(U)(\\d{4})......s II 6/17/2014\nval reg3 =......(\\d+)(-)(\\d+)(-)(\\d{4})\"\"\".r//22-08-2013\nval reg4 =......(\\w{3})\\s(\\d+)(,)\\s(\\d{4})......s II Jan 11, 2015\n\nStep 3 : Load the file as an RDD.\nval feedbackRDD = sc.textFile(\"spark9/feedback.txt\"}\n\nStep 4 : As data are pipe separated , hence split the same. \nval feedbackSplit = feedbackRDD.map(line => line.split('|'))\n\nStep 5 : Now get the valid records as well as , bad records.\nval validRecords = feedbackSplit.filter(x => (\nreg1.pattern.matcher(x(1).trim).matches |reg2.pattern.matcher(x(1).trim).matches|\nreg3.pattern.matcher(x(1).trim).matches | reg4.pattern.matcher(x(1).trim).matches)) \n\nval badRecords = feedbackSplit.filter(x =>!(\nreg1.pattern.matcher(x(1).trim).matches |reg2.pattern.matcher(x(1).trim).matches|\nreg3.pattern.matcher(x(1).trim).matches | reg4.pattern.matcher(x(1).trim).matches))\n\nStep 6 : Now convert each Array to Strings\nval valid = vatidRecords.map(e => (e(0),e(1),e(2)))\nval bad   = badRecords.map(e => (e(0),e(1),e(2)))\n\nStep 7 : Save the output as a Text file and output must be written in a single tile,\nvalid.repartition(1).saveAsTextFile(\"spark9/good.txt\")\nbad.repartition(1).saveAsTextFile(\"sparkS7bad.txt\")"],"metadata":{}},{"cell_type":"markdown","source":["#### Step 1 : \nCreate a file first using Hue in hdfs."],"metadata":{}},{"cell_type":"markdown","source":["#### Step 2 : \nWrite all valid regular expressions sysntex for checking whether records are having valid dates or not."],"metadata":{}},{"cell_type":"code","source":["val reg1 = \"\"\"(\\d+)\\s(\\w{3})(,)\\s(\\d{4})\"\"\".r // 11 Jan, 2015 \nval reg2 = \"\"\"(\\d+)\\/(\\d+)\\/(\\d{4})\"\"\".r    // 6/17/2014 \nval reg3 = \"\"\"(\\d+)(-)(\\d+)(-)(\\d{4})\"\"\".r    //  22-08-2013 \nval reg4 = \"\"\"(\\w{3})\\s(\\d+)(,)\\s(\\d{4})\"\"\".r // Jan 11, 2015"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### Step 3 : \nLoad the file as an RDD."],"metadata":{}},{"cell_type":"code","source":["val feedbackRDD = sc.textFile( \"/user/cloudera/p40/feedback.txt\" )"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["#### Step 4 : \nAs data are pipe separated , hence split the same."],"metadata":{}},{"cell_type":"code","source":["val feedbackSplit = feedbackRDD.map(line => line.split('|'))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["#### Step 5 : \nNow get the valid records as well as , bad records."],"metadata":{}},{"cell_type":"code","source":["val validRecords = feedbackSplit.filter(x => ( reg1.pattern.matcher(x(1).trim).matches |reg2.pattern.matcher(x(1).trim).matches| reg3.pattern.matcher(x(1).trim).matches | reg4.pattern.matcher(x(1).trim).matches) )"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["val badRecords = feedbackSplit.filter(x => !( reg1.pattern.matcher(x(1).trim).matches |reg2.pattern.matcher(x(1).trim).matches| reg3.pattern.matcher(x(1).trim).matches | reg4.pattern.matcher(x(1).trim).matches) )"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["#### Step 6 : \nNow convert each Array to Strings"],"metadata":{}},{"cell_type":"code","source":["val valid = vatidRecords.map(e => (e(0),e(1),e(2))) \nval bad = badRecords.map(e => (e(0),e(1),e(2)))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["#### Step 7 : Save the output as a Text file and output must be written in a single tile,"],"metadata":{}},{"cell_type":"code","source":["valid.repartition(1).saveAsTextFile( \"p40/good.txt\" ) \nbad  .repartition(1).saveAsTextFile( \"p40/bad.txt\"  )"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### Solution in in pyspark"],"metadata":{}},{"cell_type":"code","source":["import re"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["load data"],"metadata":{}},{"cell_type":"code","source":["txt = ['Christopher|Jan 11, 2015|5',\n'Kapil|11 Jan, 2015|5',\n'Thomas|6/17/2014|5',\n'John|22-08-2013|5',\n'Mithun|2013|5',\n'Jitendra||5' ]\ndata = sc.parallelize( txt ).map( lambda line: line.split( '|' )  )\ndata.take( 10 )"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["data = sc.textFile( '/user/cloudera/p40/feedback.txt' ).map( lambda line: line.split( '|' )  )\ndata.take( 10 )"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["r1 = re.compile( '([A-Za-z]{3})\\s+(\\d{1,2}),\\s+(\\d{4})' )\nr2 = re.compile( '(\\d{1,2})\\s([A-Za-z]{3}),\\s+(\\d{4})'  )\nr3 = re.compile( '(\\d{1,2})\\/(\\d{1,2})\\/(\\d{4})'        )\nr4 = re.compile( '(\\d{1,2})-(\\d{1,2})-(\\d{4})'          )"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["good = data.filter( lambda t:   r1.match( t[1] ) or r2.match( t[1] ) or r3.match( t[1] ) or r4.match( t[1] ) )\nbad  = data.filter( lambda t: not (r1.match( t[1] ) or r2.match( t[1] ) or r3.match( t[1] ) or r4.match( t[1] ) ) )"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["good.repartition( 1 ).saveAsTextFile( '/user/cloudera/p40/good.txt' )\nbad .repartition( 1 ).saveAsTextFile( '/user/cloudera/p40/bad.txt'  )"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["end\n---"],"metadata":{}}],"metadata":{"name":"p40","notebookId":2449944589270616},"nbformat":4,"nbformat_minor":0}
