{"cells":[{"cell_type":"markdown","source":["# CCA175 Practice Exam\n* Exam    : CCA175\n* Title   : CCA Spark and Hadoop Developer Exam\n* Vendor  : Cloudera\n* Version : V12.35\n\nIT Certification Guaranteed, The Easy Way!"],"metadata":{}},{"cell_type":"markdown","source":["## NO.22 CORRECT TEXT\n### Problem Scenario 31 : \n\nYou have given following two files\n\n1. Content.txt: Contain a huge text file containing space separated words.\n2. Remove.txt: Ignore/filter all the words given in this file (Comma Separated).\n\nWrite a Spark program which reads the Content.txt file and load as an RDD, remove all the words\nfrom a broadcast variables (which is loaded as an RDD of words from Remove.txt).\nAnd count the occurrence of the each word and save it as a text file in HDFS."],"metadata":{}},{"cell_type":"markdown","source":["Content.txt"],"metadata":{}},{"cell_type":"code","source":["Hello this is ABCTech.com\nThis is TechABY.com\nApache Spark Training\nThis is Spark Learning Session\nSpark is faster than MapReduce"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Remove.txt"],"metadata":{}},{"cell_type":"code","source":["Hello, is, this, the"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["#### Answer"],"metadata":{}},{"cell_type":"code","source":["See the explanation for Step by Step Solution and configuration.\nExplanation:\nSolution (in scala):\n\nStep 1 : Create all three files in hdfs in directory called spark2 (We will do using Hue).\nHowever, you can first create in local filesystem and then upload it to hdfs\n\nStep 2 : Load the Content.txt file\nval content = sc.textFile(\"spark2/Content.txt\") //Load the text file\n\nStep 3 : Load the Remove.txt file\nval remove = sc.textFile(\"spark2/Remove.txt\") //Load the text file\n\nStep 4 : Create an RDD from remove, However, there is a possibility each word could have trailing\nspaces, remove those whitespaces as well. We have used two functions here flatMap, map and trim.\nval removeRDD = remove.flatMap( x=> x.split( \",\") ).map(word=>word.trim)//Create an array of words\n\nStep 5 : Broadcast the variable, which you want to ignore\nval bRemove = sc.broadcast(removeRDD.collect().toList) // It should be array of Strings\n                              \nStep 6 : Split the content RDD, so we can have Array of String. \nval words = content.flatMap(line => line.split(\" \"))\n                              \nStep 7 : Filter the RDD, so it can have only content which are not present in \"Broadcast\nVariable\". \nval filtered = words.filter( case (word) => !bRemove.value.contains(word) )\n\nStep 8 : Create a PairRDD, so we can have (word,1) tuple or PairRDD. \nval pairRDD = filtered.map( word => (word,1) )\n\nStep 9 : Nowdo the word count on PairRDD. val wordCount = pairRDD.reduceByKey(_ + _)\n\nStep 10 : Save the output as a Text file.\nwordCount.saveAsTextFile(\"spark2/result.txt\")\n                              "],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["solution in pyspark"],"metadata":{}},{"cell_type":"code","source":["content_rdd = sc.textFile( 'spark2/Content.txt' )\n            .flatMap( lambda line: line.split( ' ' ) )\n            .map    ( lambda s: s.strip() )\n            .filter ( lambda s: s!= None and s!= '' and s!= ' ' )\n\nremove_rdd  = sc.textFile( 'spark2/Remove.txt'  )\n            .flatMap( lambda line: line.split( ',' ) )\n            .map    ( lambda s: s.strip() )\n            .filter ( lambda s: s!= None and s!= '' and s!= ' ' )\n\nb_remove = sc.broadcast( remove_rdd.collect() )"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["filtered = content_rdd.filter( lambda word: word not in b_remove.value )"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# count words. \nword_count = filtered.map( lambda word: (word, 1)  ).reduceByKey( lambda v1, v2: v1 + v2 )\n\n# count words and sort, most common word t top.\nword_count = filtered.map( lambda word: (word, 1)  ).reduceByKey( lambda v1, v2: v1 + v2 ).map( lambda (w,n): (n, w) ).sortByKey( False)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["word_count.saveAsTextFile( 'spark2/result.txt' )"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["end\n---"],"metadata":{}}],"metadata":{"name":"p22","notebookId":918720740855638},"nbformat":4,"nbformat_minor":0}
