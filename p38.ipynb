{"cells":[{"cell_type":"markdown","source":["# CCA175 Practice Exam\n* Exam    : CCA175\n* Title   : CCA Spark and Hadoop Developer Exam\n* Vendor  : Cloudera\n* Version : V12.35\n\nIT Certification Guaranteed, The Easy Way!"],"metadata":{}},{"cell_type":"markdown","source":["## NO.38 CORRECT TEXT\n### Problem Scenario 32 : \nYou have given three files as below.\n* spark3/sparkdir1/file1.txt\n* spark3/sparkdir2/file2.txt\n* spark3/sparkdir3/file3.txt\n\nEach file contain some text."],"metadata":{}},{"cell_type":"markdown","source":["spark3/sparkdir1/file1.txt"],"metadata":{}},{"cell_type":"code","source":["Apache Hadoop is an open-source software framework written in Java for distributed storage and\ndistributed processing of very large data sets on computer clusters built from commodity hardware.\nAll the modules in Hadoop are designed with a fundamental assumption that hardware failures are\ncommon and should be automatically handled by the framework "],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["spark3/sparkdir2/file2.txt"],"metadata":{}},{"cell_type":"code","source":["The core of Apache Hadoop consists of a storage part known as Hadoop Distributed File\nSystem (HDFS) and a processing part called MapReduce. Hadoop splits files into large blocks and\ndistributes them across nodes in a cluster. To process data, Hadoop transfers packaged code for\nnodes to process in parallel based on the data that needs to be processed."],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["spark3/sparkdir3/file3.txt"],"metadata":{}},{"cell_type":"code","source":["his approach takes advantage of data locality nodes manipulating the data they have access to to\nallow the dataset to be processed faster and more efficiently than it would be in a more conventional\nsupercomputer architecture that relies on a parallel file system where computation and data are\ndistributed via high-speed networking"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Now write a Spark code in scala which will load all these three files from hdfs and do the word count\nby filtering following words. And result should be sorted by word count in reverse order.\nFilter words \n\n( \"a\", \"the\", \"an\", \"as\", \"a\", \"with\", \"this\", \"these\", \"is\", \"are\", \"in\", \"for\", \"to\", \"and\", \"The\", \"of\" )\n\nAlso please make sure you load all three files as a Single RDD (All three files must be loaded using\nsingle API call).\nYou have also been given following codec"],"metadata":{}},{"cell_type":"code","source":["import org.apache.hadoop.io.compress.GzipCodec"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Please use above codec to compress file, while saving in hdfs."],"metadata":{}},{"cell_type":"markdown","source":["#### Answer"],"metadata":{}},{"cell_type":"code","source":["See the explanation for Step by Step Solution and configuration.\nExplanation:\nSolution :\nStep 1 : Create all three files in hdfs (We will do using Hue). However, you can first create in local\nfilesystem and then upload it to hdfs.\nStep 2 : Load content from all files.\n  \nval content = sc.textFile( \n  \"spark3/sparkdir1/file1.txt,spark3/sparkdir2/file2.txt,spark3/sparkdir3/file3.txt\") //Load the text file\n\nStep 3 : Now create split each line and create RDD of words.\nval flatContent = content.flatMap(word=>word.split(\" \"))\n\nstep 4 : Remove space after each word (trim it)\nval trimmedContent = f1atContent.map(word=>word.trim)\n\nStep 5 : Create an RDD from remove, all the words that needs to be removed.\nval removeRDD = sc.parallelize(List(\"a\",\"theM,ManM, \"as\",\n\"a\",\"with\",\"this\",\"these\",\"is\",\"are'\\\"in'\\ \"for\", \"to\",\"and\",\"The\",\"of\"))\n                                    \nStep 6 : Filter the RDD, so it can have only content which are not present in removeRDD.\nval filtered = trimmedContent.subtract(removeRDD}\n\nStep 7 : Create a PairRDD, so we can have (word,1) tuple or PairRDD. \nval pairRDD = filtered.map(word => (word,1))\n\nStep 8 : Now do the word count on PairRDD. \nval wordCount = pairRDD.reduceByKey(_ +_)\n                                       \nStep 9 : Now swap PairRDD.\nval swapped = wordCount.map(item => item.swap)\n                                       \nStep 10 : Now revers order the content. \nval sortedOutput = swapped.sortByKey(false)\n                                       \nStep 11 : Save the output as a Text file. \nsortedOutput.saveAsTextFile(\"spark3/result\")\n                                       \nStep 12 : Save compressed output.\nimport org.apache.hadoop.io.compress.GzipCodec\nsortedOutput.saveAsTextFile(\"spark3/compressedresult\", classOf[GzipCodec])  "],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["// load files\nval f1 = sc.textFile( \"p38/sparkdir1/file1.txt\" )\nval f2 = sc.textFile( \"p38/sparkdir2/file2.txt\" )\nval f3 = sc.textFile( \"p38/sparkdir3/file3.txt\" )"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["join the 3 files"],"metadata":{}},{"cell_type":"code","source":["val txt = f1.union( f2 ).union( f3 )"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":[" val stopWords = List( \"a\", \"the\", \"an\", \"as\", \"a\", \"with\", \"this\", \"these\", \"is\", \"are\", \"in\", \"for\", \"to\", \"and\", \"The\", \"of\" )"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["val words = txt.flatMap( _.split( \" \" ) ).map( _.trim ).filter( w => !stopWords.contains( w ) ).filter( _ != \"\" ) "],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["val count_w = words.map( w => ( w, 1) ).reduceByKey( (x, y) => x + y ).map{ case (w, n) => (n, w) }.sortByKey( false )"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["save as text file"],"metadata":{}},{"cell_type":"code","source":["count_w.saveAsTextFile( \"/user/cloudera/p38/result\" )"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Save compressed output."],"metadata":{}},{"cell_type":"code","source":["import org.apache.hadoop.io.compress.GzipCodec\ncount_w.saveAsTextFile(\"/user/cloudera/p38/compressedresult\", classOf[GzipCodec]) "],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"p38","notebookId":445118608749375},"nbformat":4,"nbformat_minor":0}
