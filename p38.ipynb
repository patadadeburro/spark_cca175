{"cells":[{"cell_type":"markdown","source":["# CCA175 Practice Exam\n* Exam    : CCA175\n* Title   : CCA Spark and Hadoop Developer Exam\n* Vendor  : Cloudera\n* Version : V12.35\n\nIT Certification Guaranteed, The Easy Way!"],"metadata":{}},{"cell_type":"markdown","source":["## NO.38 CORRECT TEXT\n### Problem Scenario 32 : \nYou have given three files as below.\n* spark3/sparkdir1/file1.txt\n* spark3/sparkdir2/file2.txt\n* spark3/sparkdir3/file3.txt\n\nEach file contain some text."],"metadata":{}},{"cell_type":"markdown","source":["spark3/sparkdir1/file1.txt"],"metadata":{}},{"cell_type":"code","source":["Apache Hadoop is an open-source software framework written in Java for distributed storage and\ndistributed processing of very large data sets on computer clusters built from commodity hardware.\nAll the modules in Hadoop are designed with a fundamental assumption that hardware failures are\ncommon and should be automatically handled by the framework "],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["spark3/sparkdir2/file2.txt"],"metadata":{}},{"cell_type":"code","source":["The core of Apache Hadoop consists of a storage part known as Hadoop Distributed File\nSystem (HDFS) and a processing part called MapReduce. Hadoop splits files into large blocks and\ndistributes them across nodes in a cluster. To process data, Hadoop transfers packaged code for\nnodes to process in parallel based on the data that needs to be processed."],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["spark3/sparkdir3/file3.txt"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["his approach takes advantage of data locality nodes manipulating the data they have access to to\nallow the dataset to be processed faster and more efficiently than it would be in a more conventional\nsupercomputer architecture that relies on a parallel file system where computation and data are\ndistributed via high-speed networking"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Now write a Spark code in scala which will load all these three files from hdfs and do the word count\nby filtering following words. And result should be sorted by word count in reverse order.\nFilter words \n\n( \"a\", \"the\", \"an\", \"as\", \"a\", \"with\", \"this\", \"these\", \"is\", \"are\", \"in\", \"for\", \"to\", \"and\", \"The\", \"of\" )\n\nAlso please make sure you load all three files as a Single RDD (All three files must be loaded using\nsingle API call).\nYou have also been given following codec"],"metadata":{}},{"cell_type":"code","source":["import org.apache.hadoop.io.compress.GzipCodec"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Please use above codec to compress file, while saving in hdfs."],"metadata":{}},{"cell_type":"markdown","source":["#### Answer"],"metadata":{}}],"metadata":{"name":"p38","notebookId":445118608749375},"nbformat":4,"nbformat_minor":0}
