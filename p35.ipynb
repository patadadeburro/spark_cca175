{"cells":[{"cell_type":"markdown","source":["# CCA175 Practice Exam\n* Exam    : CCA175\n* Title   : CCA Spark and Hadoop Developer Exam\n* Vendor  : Cloudera\n* Version : V12.35\n\nIT Certification Guaranteed, The Easy Way!"],"metadata":{}},{"cell_type":"markdown","source":["## NO.35 CORRECT TEXT\n## Problem Scenario 91 : \nYou have been given data in json format as below."],"metadata":{}},{"cell_type":"code","source":["{\"first_name\":\"Ankit\", \"last_name\":\"Jain\"}\n{\"first_name\":\"Amir\", \"last_name\":\"Khan\"}\n{\"first_name\":\"Rajesh\", \"last_name\":\"Khanna\"}\n{\"first_name\":\"Priynka\", \"last_name\":\"Chopra\"}\n{\"first_name\":\"Kareena\", \"last_name\":\"Kapoor\"}\n{\"first_name\":\"Lokesh\", \"last_name\":\"Yadav\"}"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Do the following activity\n\n1. create `employee.json` tile locally.\n2. Load this tile on hdfs\n3. Register this data as a temp table in Spark using Python.\n4. Write select query and print this data.\n5. Now save back this selected data in json format."],"metadata":{}},{"cell_type":"markdown","source":["Answer from the book looks wird:\nSee the explanation for Step by Step Solution and configuration.\nExplanation:\nSolution :\nStep 1 : create employee.json tile locally.\nvi employee.json (press insert) past the content.\nStep 2 : Upload this tile to hdfs, default location hadoop fs -put employee.json val employee =\nsqlContext.read.json(\"/user/cloudera/employee.json\") employee.write.parquet(\"employee.\nparquet\") val parq_data = sqlContext.read.parquet(\"employee.parquet\")\nparq_data.registerTempTable(\"employee\")\nval allemployee = sqlContext.sql(\"SELeCT' FROM employee\")\nall_employee.show()\nimport org.apache.spark.sql.SaveMode prdDF.write..format(\"orc\").saveAsTable(\"product ore table\"}\n//Change the codec.\nsqlContext.setConf(\"spark.sql.parquet.compression.codec\",\"snappy\")\nemployee.write.mode(SaveMode.Overwrite).parquet(\"employee.parquet\")"],"metadata":{}},{"cell_type":"markdown","source":["#### Answer"],"metadata":{}},{"cell_type":"markdown","source":["load data"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SQLContext, Row\nimport pyspark.sql.*\nsqlContext = SQLContext( sc )\nemployee = sqlContext.read.json( '/user/cloudera/p35/employee.json' )"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["register table"],"metadata":{}},{"cell_type":"code","source":["employee.registerTempTable( 'employee' )"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["make query"],"metadata":{}},{"cell_type":"code","source":["sqlContext.sql( 'SELECT * FROM employee' ).show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["write back to hdfs as json file"],"metadata":{}},{"cell_type":"code","source":["employee.write.format( 'json' ).save( '/user/cloudera/p35/results.json' )"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["end\n---"],"metadata":{}}],"metadata":{"name":"p35","notebookId":3945810008535743},"nbformat":4,"nbformat_minor":0}
