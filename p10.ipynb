{"cells":[{"cell_type":"markdown","source":["# CCA175 Practice Exam\n* Exam    : CCA175\n* Title   : CCA Spark and Hadoop Developer Exam\n* Vendor  : Cloudera\n* Version : V12.35\n\nIT Certification Guaranteed, The Easy Way!"],"metadata":{}},{"cell_type":"markdown","source":["## NO.10 CORRECT TEXT\n### Problem Scenario 46 : \n\nYou have been given below list in scala  `(name,sex,cost)` for each work done."],"metadata":{}},{"cell_type":"code","source":["List( \n(\"Deeapak\" , \"male\", 4000), \n(\"Deepak\" , \"male\", 2000), \n(\"Deepika\" , \"female\",2000),\n(\"Deepak\" , \"female\", 2000), \n(\"Deepak\" , \"male\", 1000) , \n(\"Neeta\" , \"female\", 2000))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Now write a Spark program to load this list as an RDD and do the sum of cost for combination of\nname and sex (as key)"],"metadata":{}},{"cell_type":"markdown","source":["#### Answer"],"metadata":{}},{"cell_type":"markdown","source":["See the explanation for Step by Step Solution and configuration.\nExplanation:\nSolution :"],"metadata":{}},{"cell_type":"markdown","source":["#### Step 1 : \nCreate an RDD out of this list"],"metadata":{}},{"cell_type":"code","source":["val rdd = sc.parallelize(List( (\"Deeapak\" , \"male\", 4000), (\"Deepak\" , \"male\", 2000), (\"Deepika\" , \"female\", 2000),(\"Deepak\" , \"female\", 2000), (\"Deepak\" , \"male\", 1000) , (\"Neeta\" , \"female\", 2000) ) )"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["#### Step 2 : \nConvert this RDD in pair RDD"],"metadata":{}},{"cell_type":"code","source":["val byKey = rdd.map( { case (name, sex, cost) => (name, sex) -> cost } )"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### Step 3 : \nNow group by Key"],"metadata":{}},{"cell_type":"code","source":["val byKeyGrouped = byKey.groupByKey"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["#### Step 4 : \nNowsum the cost for each group"],"metadata":{}},{"cell_type":"code","source":["val result = byKeyGrouped.map(case ((id1,id2),values) => (id1,id2,values.sum) )"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["#### Step 5 : \nSave the results"],"metadata":{}},{"cell_type":"code","source":["result.repartition(1).saveAsTextFile(\"spark12/result.txt\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["AAT. another way to do it"],"metadata":{}},{"cell_type":"code","source":["// AAT. This is another way to do it\n\nval data = List( (\"Deeapak\" , \"male\", 4000), (\"Deepak\" , \"male\", 2000), (\"Deepika\" , \"female\",2000), (\"Deepak\" , \"female\", 2000), (\"Deepak\" , \"male\", 1000), (\"Neeta\" , \"female\", 2000))\n\n// create rdd\nval rdd = sc.parallelize( data )\n\n// create key value pair rdd\nval kv = rdd.map( t => ( ( t._1, t._2), t._3 ) )\n\n// make sum\nval result = kv.reduceByKey( ( a, b ) => a + b )\n\n// save result\nresult.repartition(1).saveAsTextFile(\"spark12/result.txt\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["end\n---"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":20}],"metadata":{"name":"p10","notebookId":374374273406287},"nbformat":4,"nbformat_minor":0}
