{"cells":[{"cell_type":"markdown","source":["# CCA175 Practice Exam\n* Exam    : CCA175\n* Title   : CCA Spark and Hadoop Developer Exam\n* Vendor  : Cloudera\n* Version : V12.35\n\nIT Certification Guaranteed, The Easy Way!"],"metadata":{}},{"cell_type":"markdown","source":["## NO.2 CORRECT TEXT\n\n### Problem Scenario 81 : \nYou have been given MySQL DB with following details. You have been given following product.csv file product.csv \n\n\nproductID,productCode,name,quantity,price\n\n* 1001,PEN,Pen Red,5000,1.23\n* 1002,PEN,Pen Blue,8000,1.25\n* 1003,PEN,Pen Black,2000,1.25\n* 1004,PEC,Pencil 2B,10000,0.48\n* 1005,PEC,Pencil 2H,8000,0.49\n* 1006,PEC,Pencil HB,0,9999.99\n\nNow accomplish following activities.\n1. Create a Hive ORC table using SparkSql\n2. Load this data in Hive table.\n3. Create a Hive parquet table using SparkSQL and load data in it."],"metadata":{}},{"cell_type":"markdown","source":["#### Answer"],"metadata":{}},{"cell_type":"markdown","source":["See the explanation for Step by Step Solution and configuration.\nExplanation:\n\nSolution :\n\nStep 1 : Create this tile in HDFS under following directory (Without header}\n/user/cloudera/he/exam/task1/productcsv\n\nStep 2 : Now using Spark-shell read the file as RDD\n// load the data into a new RDD\nval products = sc.textFile(\"/user/cloudera/he/exam/task1/product.csv\")\n// Return the first element in this RDD\nprod u cts.fi rst()\n\nStep 3 : Now define the schema using a case class\ncase class Product(productid: Integer, code: String, name: String, quantity:lnteger, price:\nFloat)\n\nStep 4 : create an RDD of Product objects\nval prdRDD = products.map(_.split(\",\")).map(p =>\nProduct(p(0).tolnt,p(1),p(2),p(3}.tolnt,p(4}.toFloat))\nprdRDD.first()\nprdRDD.count()\n\nStep 5 : Now create data frame val prdDF = prdRDD.toDF()\n\nStep 6 : Now store data in hive warehouse directory. (However, table will not be created } import\norg.apache.spark.sql.SaveMode\nprdDF.write.mode(SaveMode.Overwrite).format(\"orc\").saveAsTable(\"product_orc_table\") \n\nstep 7:\nNow create table using data stored in warehouse directory. With the help of hive.\nhive\nshow tables\nCREATE EXTERNAL TABLE products (productid int,code string,name string .quantity int, price float}\nSTORED AS ore\nLOCATION 7user/hive/warehouse/product_orc_table';\n\nStep 8 : Now create a parquet table\nimport org.apache.spark.sql.SaveMode\nprdDF.write.mode(SaveMode.Overwrite).format(\"parquet\").saveAsTable(\"product_parquet_ table\")\n\nStep 9 : Now create table using this\nCREATE EXTERNAL TABLE products_parquet (productid int,code string,name string\n.quantity int, price float}\nSTORED AS parquet\nLOCATION 7user/hive/warehouse/product_parquet_table';\n\nStep 10 : Check data has been loaded or not.\nSelect * from products;\nSelect * from products_parquet;"],"metadata":{}},{"cell_type":"markdown","source":["## Step 1:\nCreate this tile in HDFS under following directory (Without header) /user/cloudera/he/exam/task1/productcsv\n\nLinux console."],"metadata":{}},{"cell_type":"code","source":["# go to local file path\ncd /home/training/training_materials/data\n\n# copy data products.csv to hadoop\nhdfs dfs -put product.csv /user/cloudera/he/exam/task1/product.csv\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["## Step 2 : \nNow using Spark-shell read the file as RDD"],"metadata":{}},{"cell_type":"code","source":["// spark-shell\n// load the data into a new RDD \nval products = sc.textFile(\"/user/cloudera/he/exam/task1/product.csv\") \n\n// Return the first element in this RDD \nproducts.first()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# pyspark\n# load the data into a new RDD \nproducts = sc.textFile(\"/user/cloudera/he/exam/task1/product.csv\") \n\n# Return the first element in this RDD \nproducts.first()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Step 3 : \nNow define the schema using a case class.\n\nspark-shell"],"metadata":{}},{"cell_type":"code","source":["// spark-shell\ncase class Product( productid: Integer, code: String, name: String, quantity: Integer, price: Float )"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# pyspark\nProduct = Row( 'productid', 'code', 'name', 'quantity', 'price' )"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## Step 4 : \ncreate an RDD of Product objects \n\nspark-shell"],"metadata":{}},{"cell_type":"code","source":["val prdRDD = products\n              .map( _.split(\",\") )\n              .map( p => Product( p(0).toInt, p(1), p(2), p(3).toInt, p(4).toFloat) )\n    \nprdRDD.first() \nprdRDD.count()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#pyspark\nprdRDD = products.map( lambda line: line.split( ',' ) ).map( lambda p: Product( *p ) )"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["##Step 5 : \nNow create data frame"],"metadata":{}},{"cell_type":"code","source":[" // spark-shell\n  val prdDF = prdRDD.toDF()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#pyspark\nprdDF = prdRDD.toDF()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## Step 6 : \nNow store data in hive warehouse directory. (However, table will not be created }"],"metadata":{}},{"cell_type":"code","source":["import org.apache.spark.sql.SaveMode \nprdDF.write.mode(SaveMode.Overwrite).format(\"orc\").saveAsTable(\"product_orc_table\") "],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## step 7:\nNow create table using data stored in warehouse directory. With the help of hive.\n\nhive console"],"metadata":{}},{"cell_type":"code","source":["hive\n\nshow tables;\nCREATE EXTERNAL TABLE products ( productid int, code string, name string, quantity int, price float )\nSTORED AS orc \nLOCATION '/user/hive/warehouse/product_orc_table';\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Step 8 : \nNow create a parquet table.\nspark-shell"],"metadata":{}},{"cell_type":"code","source":["import org.apache.spark.sql.SaveMode \nprdDF.write.mode( SaveMode.Overwrite ).format( \"parquet\" ).saveAsTable( \"product_parquet_table\" )"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["##Step 9 : \nNow create table using this \nhive console"],"metadata":{}},{"cell_type":"code","source":["CREATE EXTERNAL TABLE products_parquet (\n  productid int,\n  code string,\n  name string,\n  quantity int, \n  price float)\nSTORED AS parquet \nLOCATION '/user/hive/warehouse/product_parquet_table';"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["## Step 10 : \nCheck data has been loaded or not. \nhive console"],"metadata":{}},{"cell_type":"code","source":["Select from products; "],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["Select from products_parquet;"],"metadata":{},"outputs":[],"execution_count":29}],"metadata":{"name":"p02","notebookId":3275581133599536},"nbformat":4,"nbformat_minor":0}
