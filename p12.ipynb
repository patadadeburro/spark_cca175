{"cells":[{"cell_type":"markdown","source":["# CCA175 Practice Exam\n* Exam    : CCA175\n* Title   : CCA Spark and Hadoop Developer Exam\n* Vendor  : Cloudera\n* Version : V12.35\n\nIT Certification Guaranteed, The Easy Way!"],"metadata":{}},{"cell_type":"markdown","source":["## NO.12 CORRECT TEXT\n### Problem Scenario 69 : \n\nWrite down a Spark Application using Python. \nIn which it read a file \"Content.txt\" (On hdfs) with following content.\nContent.txt"],"metadata":{}},{"cell_type":"code","source":["Hello this is ABCTECH.com\nThis is ABYTECH.com\nApache Spark Training\nThis is Spark Learning Session\nSpark is faster than MapReduce"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["* And filter out the word which is less than 2 characters and \n* ignore all empty lines.\n* Once done store the filtered data in a directory called \"problem84\" (On hdfs)"],"metadata":{}},{"cell_type":"markdown","source":["#### Answer"],"metadata":{}},{"cell_type":"markdown","source":["the solution below save the data as a list of words"],"metadata":{}},{"cell_type":"markdown","source":["#### Step 1 : \nCreate an application with following code and store it in problem84.py"],"metadata":{}},{"cell_type":"code","source":["# problem84.py\n\n# import SparkContext SparkConf\nfrom pyspark import SparkContext, SparkConf\n\n# Create configuration object and set App name\nconf = SparkConf().setAppName( \"CCA 175 Problem 84\" ) \nsc   = SparkContext(conf=conf)\n#sc  = SparkContext()\n\ncontentRDD = sc.textFile( '/user/cloudera/Content.txt' )\n\n#filter out non-empty lines\nnon_empty_lines = contentRDD.filter( lambda x: len(x) > 0 )\n\n#Split line based on space\nwords = non_empty_lines.flatMap( lambda x: x.split( ' ' ) )\n                                                 \n#filter out all 2 letter words\nfinalRDD = words.filter(lambda x: len(x) > 2)\n\nfor word in finalRDD.collect():\n  print(word)\n\n#Save final data \nfinalRDD.saveAsTextFile( 'problem84' )"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["#### step 2 : \nSubmit this application"],"metadata":{}},{"cell_type":"code","source":["spark-submit --master yarn problem84.py"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["AAT solution save a file where each line can contain many words."],"metadata":{}},{"cell_type":"code","source":["# AAT.\n# p12.py\n\n# import SparkContext SparkConf\nfrom pyspark import SparkContext, SparkConf\n\n# Create configuration object and set App name\nsc = SparkContext()\n\n# load file\ncontentRDD = sc.textFile( '/user/cloudera/Content.txt' )\n\ndef filter_small_words( line, word_size ):\n  a = line.split( ' ' )\n  s = ''\n  \n  for i in a:\n    \n    if i == None or len( i.strip() ) < word_size:\n      continue\n    word = i.strip()\n    \n    if s == '':\n      s = word\n    else:\n      s = s + ' ' + word\n      \n  return s \n\n# filter out words less than 2 characters\nword_size = 2\nwords = contentRDD.map( lambda line: filter_small_words( line, word_size ) )\n\n# filter out empty lines\nnon_empty_lines = words.filter( lambda row: len( row.strip() ) >= word_size  )\n\n# save clean file\nnon_empty_lines.saveAsTextFile( 'problem84_aat' )"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["submit application"],"metadata":{}},{"cell_type":"code","source":["spark-submit --master yarn p12.py"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["end\n---"],"metadata":{}}],"metadata":{"name":"p12","notebookId":4216584261369843},"nbformat":4,"nbformat_minor":0}
