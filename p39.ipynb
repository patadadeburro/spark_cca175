{"cells":[{"cell_type":"markdown","source":["# CCA175 Practice Exam\n* Exam    : CCA175\n* Title   : CCA Spark and Hadoop Developer Exam\n* Vendor  : Cloudera\n* Version : V12.35\n\nIT Certification Guaranteed, The Easy Way!"],"metadata":{}},{"cell_type":"markdown","source":["## NO.39 CORRECT TEXT\n### Problem Scenario 80 : \n\nYou have been given MySQL DB with following details.\n\n|||\n|-|-|\n|user     | retail_dba |\n|password | cloudera   |\n|database | retail_db  |\n|table    | retail_db.products |\n|jdbc URL |  jdbc:mysql://quickstart:3306/retail_db|\n\nColumns of products table : \n\n`(product_id | product_category_id | product_name | product_description | product_price | product_image )`\n\nPlease accomplish following activities.\n1. Copy `retaildb.products` table to hdfs in a directory p39\n2. Now sort the products data sorted by product price per category, use `product_category_id` column to group by category"],"metadata":{}},{"cell_type":"markdown","source":["Solution from book is wird\n\nSee the explanation for Step by Step Solution and configuration.\nExplanation:\nSolution :\nStep 1 : Import Single table .\nsqoop import --connect jdbc:mysql://quickstart:3306/retail_db -username=retail_dba -\npassword=cloudera -table=products --target-dir=p93\nNote : Please check you dont have space between before or after '=' sign. Sqoop uses the\nMapReduce framework to copy data from RDBMS to hdfs\n\nStep 2 : Step 2 : Read the data from one of the partition, created using above command, hadoop fs -\ncat p93_products/part-m-00000\nStep 3 : Load this directory as RDD using Spark and Python (Open pyspark terminal and do following}.\nproductsRDD = sc.textFile(Mp93_products\")\nStep 4 : Filter empty prices, if exists\n\n--filter out empty prices lines\nNonempty_lines = productsRDD.filter(lambda x: len(x.split(\",\")[4]) > 0)\n                          \nStep 5 : Create data set like (categroyld, (id,name,price)\nmappedRDD = nonempty_lines.map(\n  lambda line: \n  (   \n    line.split(\",\")[1], \n    ( line.split(\",\")[0], line.split(\",\")[2], float(line.split(\",\")[4]) ) \n  )\n) \nfor line in mappedRDD.collect(): print(line)\n                               \nStep 6 : Now groupBy the all records based on categoryld, which a key on mappedRDD it will produce\noutput like (categoryld, iterable of all lines for a key/categoryld) \ngroupByCategroyld =mappedRDD.groupByKey() \nfor line in groupByCategroyld.collect():print(line)\n                               \n                               \nsteps 7 and 8 give ERRORS                               \n                               \nstep 7 : Now sort the data in each category based on price in ascending order.\n-- sorted is a function to sort an iterable, we can also specify, what would be the Key on which we\nwant to sort in this case we have price on which it needs to be sorted.\ngroupByCategroyld.map(lambda tuple: \n                      sorted( \n                              tuple[1], \n                              key=lambda tupleValue:tupleValue[2] )\n                     )\n                 .take(5)\n                               \nStep 8 : Now sort the data in each category based on price in descending order.\n-- sorted is a function to sort an iterable, we can also specify, what would be the Key on which we\nwant to sort in this case we have price which it needs to be sorted.\non groupByCategroyld.map(lambda tuple: sorted(tuple[1], key=lambda tupleValue:\ntupleValue[2] , reverse=True)).take(5)"],"metadata":{}},{"cell_type":"markdown","source":["#### Answer.\nAAT"],"metadata":{}},{"cell_type":"markdown","source":["import data from mySQL database to hdfs"],"metadata":{}},{"cell_type":"code","source":["sqoop import\n--connect  jdbc:mysql://quickstart:3306/retail_db\n--username=retail_dba\n--password=cloudera\n--table    products\n--fields-terminated-by '\\t'\n--target-dir /user/cloudera/p39\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["open pyspark and load some libraries"],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql\nimport pyspark.sql.functions\nfrom pyspark.sql import SQLContext, Row"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["def p2(t):\n  print( 'key:{}'.format( t[0] ) )\n  for i in t[1]: print( '\\t{}'.format( i ) )"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["load data into an rdd of tuples"],"metadata":{}},{"cell_type":"code","source":["#product_id | product_category_id | product_name | product_description | product_price\n# category, ( id, name, price )\n\ntuples = sc.textFile( '/user/cloudera/p39' ).map( lambda line: line.split( '\\t' ) ).map( lambda a: ( int(a[1]), int(a[0]), a[2], float(a[4] ) ) )"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["create rdd of Data Rows"],"metadata":{}},{"cell_type":"code","source":["rows = tuples.map( lambda a: Row( product_category_id = int( a[0] ), product_id= int( a[1] ), product_name = a[2], product_price = float( a[3] ) ) )\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["create dataframe `products` and register it to use it in sql queries."],"metadata":{}},{"cell_type":"code","source":["products = rows.toDF()\nproducts.registerTempTable( 'products' )"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# sort by price in ascending\ndf1 = sqlContext.sql( \"\"\"\nSELECT product_category_id, product_id, product_name, product_price \nFROM products \nORDER BY product_category_id, product_price\n\"\"\"\n)\n\n# group\ng1 = df1.rdd.map( lambda a: ( int(a['product_category_id']), ( int(a['product_id']), a['product_name'], float(a['product_price']) ) ) ).groupByKey()\n\n# print results\nfor i in g1.take(5): p2( i )"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# sort by price in descending order\ndf2 = sqlContext.sql( \"\"\"\nSELECT product_category_id, product_id, product_name, product_price \nFROM products \nORDER BY product_category_id, product_price DESC\n\"\"\" )\n\n# group\ng2 = df2.rdd.map( lambda a: ( int(a['product_category_id']), ( int(a['product_id']), a['product_name'], float(a['product_price']) ) ) ).groupByKey()\n\n# print results\nfor i in g2.take(5): p2( i )"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["end\n---"],"metadata":{}}],"metadata":{"name":"p39","notebookId":3044667302843226},"nbformat":4,"nbformat_minor":0}
