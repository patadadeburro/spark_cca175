{"cells":[{"cell_type":"markdown","source":["# CCA175 Practice Exam\n* Exam    : CCA175\n* Title   : CCA Spark and Hadoop Developer Exam\n* Vendor  : Cloudera\n* Version : V12.35\n\nIT Certification Guaranteed, The Easy Way!"],"metadata":{}},{"cell_type":"markdown","source":["## NO.39 CORRECT TEXT\n### Problem Scenario 80 : \n\nYou have been given MySQL DB with following details.\n\n|||\n|-|-|\n|user     | retail_dba |\n|password | cloudera   |\n|database | retail_db  |\n|table    | retail_db.products |\n|jdbc URL |  jdbc:mysql://quickstart:3306/retail_db|\n\nColumns of products table : \n\n`(product_id | product_category_id | product_name | product_description | product_price | product_image )`\n\nPlease accomplish following activities.\n1. Copy `retaildb.products` table to hdfs in a directory p39\n2. Now sort the products data sorted by product price per category, use `product_category_id` column to group by category"],"metadata":{}},{"cell_type":"markdown","source":["Answer"],"metadata":{}},{"cell_type":"code","source":["See the explanation for Step by Step Solution and configuration.\nExplanation:\nSolution :\nStep 1 : Import Single table .\nsqoop import --connect jdbc:mysql://quickstart:3306/retail_db -username=retail_dba -\npassword=cloudera -table=products --target-dir=p93\nNote : Please check you dont have space between before or after '=' sign. Sqoop uses the\nMapReduce framework to copy data from RDBMS to hdfs\n\nStep 2 : Step 2 : Read the data from one of the partition, created using above command, hadoop fs -\ncat p93_products/part-m-00000\nStep 3 : Load this directory as RDD using Spark and Python (Open pyspark terminal and do following}.\nproductsRDD = sc.textFile(Mp93_products\")\nStep 4 : Filter empty prices, if exists\n#filter out empty prices lines\nNonempty_lines = productsRDD.filter(lambda x: len(x.split(\",\")[4]) > 0)\nStep 5 : Create data set like (categroyld, (id,name,price)\nmappedRDD = nonempty_lines.map(lambda line: (line.split(\",\")[1], (line.split(\",\")[0], line.split(\",\")[2],\nfloat(line.split(\",\")[4])))) tor line in mappedRDD.collect(): print(line)\nStep 6 : Now groupBy the all records based on categoryld, which a key on mappedRDD it will produce\noutput like (categoryld, iterable of all lines for a key/categoryld) groupByCategroyld =\nmappedRDD.groupByKey() for line in groupByCategroyld.collect():\nprint(line)\nstep 7 : Now sort the data in each category based on price in ascending order.\n# sorted is a function to sort an iterable, we can also specify, what would be the Key on which we\nwant to sort in this case we have price on which it needs to be sorted.\ngroupByCategroyld.map(lambda tuple: sorted(tuple[1], key=lambda tupleValue:\ntupleValue[2])).take(5)\nStep 8 : Now sort the data in each category based on price in descending order.\n# sorted is a function to sort an iterable, we can also specify, what would be the Key on which we\nwant to sort in this case we have price which it needs to be sorted.\non groupByCategroyld.map(lambda tuple: sorted(tuple[1], key=lambda tupleValue:\ntupleValue[2] , reverse=True)).take(5)\n                               "],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["import data from mySQL database to hdfs"],"metadata":{}},{"cell_type":"code","source":["sqoop import\n--connect  jdbc:mysql://quickstart:3306/retail_db\n--username=retail_dba\n--password=cloudera\n--table    products\n--fields-terminated-by '\\t'\n--target-dir /user/cloudera/p39\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["open pyspark and load some libraries"],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql\nimport pyspark.sql.functions\nfrom pyspark.sql import SQLContext, Row"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["load data into an rdd of vector of strings"],"metadata":{}},{"cell_type":"code","source":["p = sc.textFile( '/user/cloudera/p39' ).map( lambda line: line.split( '\\t' ) )\np.take( 10 )"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["create an rdd of Rows"],"metadata":{}},{"cell_type":"code","source":["rdd = p.map( lambda a: Row( product_id= int( a[0] ), product_category_id = int( a[1] ), product_name = a[2], product_description =a[3], product_price = float( a[4] ), product_image =a[5],) )"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["create dataframe"],"metadata":{}},{"cell_type":"code","source":["products = rdd.toDF()\ndf.registerTempTable( 'products' )"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["make sql query"],"metadata":{}},{"cell_type":"code","source":["results = sqlContext.sql( \"\"\"\nSELECT * \nFROM products \nORDER BY product_category_id, product_price\n\"\"\"\n)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["end\n---"],"metadata":{}}],"metadata":{"name":"p39","notebookId":3044667302843226},"nbformat":4,"nbformat_minor":0}
